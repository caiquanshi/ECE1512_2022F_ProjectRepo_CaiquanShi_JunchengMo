{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LtwgoM3ygnx",
        "outputId": "1988ed21-b7c7-46c3-9ec0-54271490a1bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting import_ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from import_ipynb) (5.7.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.8/dist-packages (from import_ipynb) (7.9.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (5.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (0.2.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->IPython->import_ipynb) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import_ipynb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import_ipynb) (0.2.5)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat->import_ipynb) (5.1.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->import_ipynb) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->import_ipynb) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (5.10.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import_ipynb) (3.11.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core->nbformat->import_ipynb) (2.5.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->IPython->import_ipynb) (0.7.0)\n",
            "Installing collected packages: jedi, import-ipynb\n",
            "Successfully installed import-ipynb-0.1.4 jedi-0.18.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.6.9.tar.gz (12 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from ptflops) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->ptflops) (4.4.0)\n",
            "Building wheels for collected packages: ptflops\n",
            "  Building wheel for ptflops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptflops: filename=ptflops-0.6.9-py3-none-any.whl size=11712 sha256=efc265e0d377bf240b5a31300bb26078ae84a67aa93f73baa71bbd5eb5b27e64\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/86/d5/cf62a3571b005f91cd9accefc5e10f40214538be997198afad\n",
            "Successfully built ptflops\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.6.9\n",
            "importing Jupyter notebook from utils.ipynb\n",
            "importing Jupyter notebook from networks.ipynb\n"
          ]
        }
      ],
      "source": [
        "!pip install import_ipynb\n",
        "!pip install ptflops\n",
        "import import_ipynb\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from ptflops import get_model_complexity_info\n",
        "from utils import get_dataset, get_network, TensorDataset, get_time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LghNE8DKgDKO"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ZqmO085f8vb"
      },
      "outputs": [],
      "source": [
        "DATAPATH = './data'\n",
        "SAVEPATH = './'\n",
        "MODEL = 'ConvNet'\n",
        "IMAGE_PER_CLASS = 10\n",
        "EVAL_MODE = 'SS'\n",
        "TRAIN_NET_EPOCH = 20\n",
        "BATCH_SIZE = 256\n",
        "LR_IMG = 1\n",
        "LR_NET = 0.01\n",
        "NUMBER_CLASSES = 10\n",
        "ITERATIONS = 10\n",
        "NUMBER_INITIALIZATIONS = 100\n",
        "NUMBER_STEP_OPT = 50\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa9UynX22VFh"
      },
      "source": [
        "Task1 2.a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "2xnbJXq-6ogG"
      },
      "outputs": [],
      "source": [
        "def evaluate(model,valid_data):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for data in valid_data:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        output = model(inputs)\n",
        "        output = torch.argmax(output, dim=1)\n",
        "        correct += torch.sum(output == labels)\n",
        "        total += BATCH_SIZE\n",
        "    return correct / float(total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532,
          "referenced_widgets": [
            "1675efe4e6db4f3a8fa0ae25aead0558",
            "10aace68830445a48f6bba04eb45e774",
            "ec75cd3d930e4c9080dee52cd8087e8b",
            "1fcccd7fdf4f4363b5b74d871cfd5b34",
            "f1fb9e67685a4950b6653ac4da288774",
            "3f3b5d44fe9a40a5b683c35439e630ee",
            "770b450752bb43c4a5c4f92240dea42a",
            "ea519938a6784e4ab66615f2ba346780",
            "d66d787fbf7146f6ad8efa9f125dce63",
            "048da1980d71475d98fed0fb3cf74689",
            "cb9ce157239b4b25aee114f70843c723"
          ]
        },
        "id": "zYSL45wL2URA",
        "outputId": "16abdc13-34b2-47b9-da9b-a69d38353034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1675efe4e6db4f3a8fa0ae25aead0558"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "[2022-12-12 07:52:16] training begins\n",
            "Epoch: 0 \tTraining Cost: 359.247167\t Valid Acc: 0.44941407442092896.\n",
            "Epoch: 1 \tTraining Cost: 286.007982\t Valid Acc: 0.52392578125.\n",
            "Epoch: 2 \tTraining Cost: 249.559107\t Valid Acc: 0.567089855670929.\n",
            "Epoch: 3 \tTraining Cost: 226.537605\t Valid Acc: 0.57373046875.\n",
            "Epoch: 4 \tTraining Cost: 210.482247\t Valid Acc: 0.610546886920929.\n",
            "Epoch: 5 \tTraining Cost: 197.720117\t Valid Acc: 0.615527331829071.\n",
            "Epoch: 6 \tTraining Cost: 187.176338\t Valid Acc: 0.623242199420929.\n",
            "Epoch: 7 \tTraining Cost: 179.077719\t Valid Acc: 0.6280273795127869.\n",
            "Epoch: 8 \tTraining Cost: 170.550790\t Valid Acc: 0.652148425579071.\n",
            "Epoch: 9 \tTraining Cost: 163.824359\t Valid Acc: 0.6416015625.\n",
            "Epoch: 10 \tTraining Cost: 157.849175\t Valid Acc: 0.6825195550918579.\n",
            "Epoch: 11 \tTraining Cost: 152.773350\t Valid Acc: 0.683789074420929.\n",
            "Epoch: 12 \tTraining Cost: 147.825725\t Valid Acc: 0.6783203482627869.\n",
            "Epoch: 13 \tTraining Cost: 142.858489\t Valid Acc: 0.6953125.\n",
            "Epoch: 14 \tTraining Cost: 138.996155\t Valid Acc: 0.663769543170929.\n",
            "Epoch: 15 \tTraining Cost: 134.286999\t Valid Acc: 0.675488293170929.\n",
            "Epoch: 16 \tTraining Cost: 130.961806\t Valid Acc: 0.706347644329071.\n",
            "Epoch: 17 \tTraining Cost: 126.926318\t Valid Acc: 0.71728515625.\n",
            "Epoch: 18 \tTraining Cost: 123.278870\t Valid Acc: 0.6761718988418579.\n",
            "Epoch: 19 \tTraining Cost: 121.373231\t Valid Acc: 0.698925793170929.\n",
            "[2022-12-12 07:58:21] training ends\n",
            "Warning: module ConvNet is treated as a zero-op.\n",
            "Valid Acc: 0.698925793170929 \tFlops: 51.61 MMac\n"
          ]
        }
      ],
      "source": [
        "channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(\"CIFAR10\", \"./data\")\n",
        "model = get_network(\"ConvNet\", channel, num_classes, im_size).to(DEVICE)\n",
        "trainloader = torch.utils.data.DataLoader(dst_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validLoader = torch.utils.data.DataLoader(dst_test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR_NET)\n",
        "val_acc_list, train_loss_list, epoch_list = [], [], []\n",
        "cee = nn.CrossEntropyLoss()\n",
        "print('%s training begins'%get_time())\n",
        "for epoch in range(0, TRAIN_NET_EPOCH):\n",
        "  model.train()\n",
        "  train_loss = 0.\n",
        "\n",
        "  for data in trainloader:\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      loss = cee(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "\n",
        "  valid_acc = evaluate(model, validLoader)\n",
        "  print(\"Epoch: {} \\tTraining Cost: {:.6f}\\t \"\n",
        "      \"Valid Acc: {}.\".format(epoch, train_loss, valid_acc))\n",
        "  val_acc_list.append(valid_acc)\n",
        "  train_loss_list.append(train_loss)\n",
        "\n",
        "print('%s training ends'%get_time())\n",
        "\n",
        "macs, params = get_model_complexity_info(model, (3, im_size[0], im_size[1]), as_strings=True,\n",
        "                                           print_per_layer_stat=False, verbose=True)\n",
        "print(\"Valid Acc: {} \\tFlops: {}\".format(valid_acc, macs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy15wU0UOn7A",
        "outputId": "30c5e1ed-75ef-4eeb-eebb-274b70f0e7e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-12-12 07:58:22] training begins\n",
            "Epoch: 0 \tTraining Cost: 130.068001\t Valid Acc: 0.941699206829071.\n",
            "Epoch: 1 \tTraining Cost: 35.584123\t Valid Acc: 0.9500976800918579.\n",
            "Epoch: 2 \tTraining Cost: 25.099387\t Valid Acc: 0.95556640625.\n",
            "Epoch: 3 \tTraining Cost: 20.181787\t Valid Acc: 0.9585937857627869.\n",
            "Epoch: 4 \tTraining Cost: 17.249245\t Valid Acc: 0.9593750238418579.\n",
            "Epoch: 5 \tTraining Cost: 15.276834\t Valid Acc: 0.9627929925918579.\n",
            "Epoch: 6 \tTraining Cost: 13.692698\t Valid Acc: 0.964160144329071.\n",
            "Epoch: 7 \tTraining Cost: 12.566437\t Valid Acc: 0.9639648795127869.\n",
            "Epoch: 8 \tTraining Cost: 11.712995\t Valid Acc: 0.9652343988418579.\n",
            "Epoch: 9 \tTraining Cost: 10.818626\t Valid Acc: 0.966601550579071.\n",
            "Epoch: 10 \tTraining Cost: 10.223773\t Valid Acc: 0.9658203125.\n",
            "Epoch: 11 \tTraining Cost: 9.717043\t Valid Acc: 0.967578113079071.\n",
            "Epoch: 12 \tTraining Cost: 9.250067\t Valid Acc: 0.96630859375.\n",
            "Epoch: 13 \tTraining Cost: 8.788854\t Valid Acc: 0.9676758050918579.\n",
            "Epoch: 14 \tTraining Cost: 8.320960\t Valid Acc: 0.966992199420929.\n",
            "Epoch: 15 \tTraining Cost: 8.111629\t Valid Acc: 0.967480480670929.\n",
            "Epoch: 16 \tTraining Cost: 7.699966\t Valid Acc: 0.964160144329071.\n",
            "Epoch: 17 \tTraining Cost: 7.360348\t Valid Acc: 0.9688476920127869.\n",
            "Epoch: 18 \tTraining Cost: 7.157166\t Valid Acc: 0.969042956829071.\n",
            "Epoch: 19 \tTraining Cost: 6.933887\t Valid Acc: 0.96826171875.\n",
            "[2022-12-12 08:04:32] training ends\n",
            "Warning: module ConvNet is treated as a zero-op.\n",
            "Valid Acc: 0.96826171875 \tFlops: 49.25 MMac\n",
            "[tensor(0.9417, device='cuda:0'), tensor(0.9501, device='cuda:0'), tensor(0.9556, device='cuda:0'), tensor(0.9586, device='cuda:0'), tensor(0.9594, device='cuda:0'), tensor(0.9628, device='cuda:0'), tensor(0.9642, device='cuda:0'), tensor(0.9640, device='cuda:0'), tensor(0.9652, device='cuda:0'), tensor(0.9666, device='cuda:0'), tensor(0.9658, device='cuda:0'), tensor(0.9676, device='cuda:0'), tensor(0.9663, device='cuda:0'), tensor(0.9677, device='cuda:0'), tensor(0.9670, device='cuda:0'), tensor(0.9675, device='cuda:0'), tensor(0.9642, device='cuda:0'), tensor(0.9688, device='cuda:0'), tensor(0.9690, device='cuda:0'), tensor(0.9683, device='cuda:0')] [130.06800085306168, 35.5841231867671, 25.099387034773827, 20.18178689107299, 17.24924463033676, 15.276834474876523, 13.692698448896408, 12.566436504945159, 11.712994566187263, 10.818626185879111, 10.223773221485317, 9.717043034732342, 9.250066596083343, 8.78885434102267, 8.32096016779542, 8.11162932869047, 7.699965747073293, 7.36034757271409, 7.157165736891329, 6.933886697050184]\n"
          ]
        }
      ],
      "source": [
        "channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(\"MNIST\", \"./data\")\n",
        "model = get_network(\"ConvNet\", channel, num_classes, im_size).to(DEVICE)\n",
        "trainloader = torch.utils.data.DataLoader(dst_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validLoader = torch.utils.data.DataLoader(dst_test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR_NET)\n",
        "val_acc_list_MNIST_ConvNet_real, train_loss_list_MNIST_ConvNet_real, epoch_list = [], [], []\n",
        "cee = nn.CrossEntropyLoss()\n",
        "print('%s training begins'%get_time())\n",
        "for epoch in range(0, TRAIN_NET_EPOCH):\n",
        "  model.train()\n",
        "  train_loss = 0.\n",
        "\n",
        "  for data in trainloader:\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      loss = cee(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "\n",
        "  valid_acc = evaluate(model, validLoader)\n",
        "  print(\"Epoch: {} \\tTraining Cost: {:.6f}\\t \"\n",
        "      \"Valid Acc: {}.\".format(epoch, train_loss, valid_acc))\n",
        "  val_acc_list_MNIST_ConvNet_real.append(valid_acc)\n",
        "  train_loss_list_MNIST_ConvNet_real.append(train_loss)\n",
        "\n",
        "print('%s training ends'%get_time())\n",
        "\n",
        "macs, params = get_model_complexity_info(model, (channel, im_size[0], im_size[1]), as_strings=True,\n",
        "                                           print_per_layer_stat=False, verbose=True)\n",
        "print(\"Valid Acc: {} \\tFlops: {}\".format(valid_acc, macs))\n",
        "print(val_acc_list_MNIST_ConvNet_real,train_loss_list_MNIST_ConvNet_real)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psCVWXoy28wv"
      },
      "source": [
        "Task1 2.b c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "bAzsnkNJA8EQ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model,valid_data):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for data in valid_data:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        output = model(inputs)\n",
        "        output = torch.argmax(output, dim=1)\n",
        "        correct += torch.sum(output == labels)\n",
        "        total += BATCH_SIZE\n",
        "    return correct / float(total)\n",
        "\n",
        "def train_and_eval(model, image_syn_eval, syn_lable, dst_test):\n",
        "  dst_train = TensorDataset(image_syn_eval, syn_lable)\n",
        "  trainloader = torch.utils.data.DataLoader(dst_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  validLoader = torch.utils.data.DataLoader(dst_test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=LR_NET)\n",
        "  cee = nn.CrossEntropyLoss()\n",
        "  valid_accs = []\n",
        "  for epoch in range(TRAIN_NET_EPOCH):\n",
        "    model.train()\n",
        "    train_loss = 0.\n",
        "\n",
        "    for data in trainloader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = cee(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    valid_acc = evaluate(model, validLoader)\n",
        "    valid_accs.append(valid_acc)\n",
        "    print(\"Epoch: {} \\tTraining Cost: {:.6f}\\t \"\n",
        "        \"Valid Acc: {}.\".format(epoch, train_loss, valid_acc))\n",
        "  return valid_accs\n",
        "\n",
        "def train_model(model, image_syn_eval, syn_lable, epochs):\n",
        "  dst_train = TensorDataset(image_syn_eval, syn_lable)\n",
        "  trainloader = torch.utils.data.DataLoader(dst_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=LR_NET)\n",
        "  cee = nn.CrossEntropyLoss().to(DEVICE)\n",
        "  for e in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.\n",
        "\n",
        "    for data in trainloader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = cee(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def match_mse_loss(gw_syn, gw_real):\n",
        "  gw_real_vec = []\n",
        "  gw_syn_vec = []\n",
        "  for ig in range(len(gw_real)):\n",
        "      gw_real_vec.append(gw_real[ig].reshape((-1)))\n",
        "      gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n",
        "  gw_real_vec = torch.cat(gw_real_vec, dim=0)\n",
        "  gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n",
        "  dis = torch.sum((gw_syn_vec - gw_real_vec) ** 2)\n",
        "  return dis\n",
        "\n",
        "\n",
        "def condensed_images_and_evaluate(dataset_name, model_name, eval_mode_name, randmon_init=False):\n",
        "  channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(dataset_name, DATAPATH)\n",
        "  class_index = {}\n",
        "  data_save = []\n",
        "  for i in range(len(dst_train)):\n",
        "    lable = dst_train[i][1]\n",
        "    if lable in class_index:\n",
        "      class_index[lable].append(i)\n",
        "    else:\n",
        "      class_index[lable] = [i]\n",
        "  images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
        "  images_all = torch.cat(images_all, dim=0).to(DEVICE)\n",
        "  syn_image = []\n",
        "  for c in range(num_classes):\n",
        "    idx_shuffle = np.random.permutation(class_index[c])[:IMAGE_PER_CLASS]\n",
        "    syn_image.append(images_all[idx_shuffle])\n",
        "  syn_image = torch.cat(syn_image, dim=0).to(DEVICE)\n",
        "  syn_image.requires_grad = True\n",
        "  syn_lable = []\n",
        "  for c in range(num_classes):\n",
        "    syn_lable += [c for i in range(IMAGE_PER_CLASS)]\n",
        "  if randmon_init:\n",
        "    syn_image = torch.randn(size=(num_classes*IMAGE_PER_CLASS, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=DEVICE)\n",
        "  syn_lable = torch.tensor(syn_lable,device=DEVICE)\n",
        "  optimizer_img = torch.optim.SGD([syn_image, ], lr=LR_IMG)\n",
        "  criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "  total_loss = 0 \n",
        "  print('%s training begins'%get_time())\n",
        "  for exp in range(NUMBER_INITIALIZATIONS):\n",
        "    model = get_network(model_name, channel, num_classes, im_size).to(DEVICE)\n",
        "    model.train()\n",
        "    net_parameters = list(model.parameters())\n",
        "\n",
        "    for it in range(ITERATIONS):\n",
        "      loss = torch.tensor(0.0).to(DEVICE)\n",
        "      for c in range(num_classes):\n",
        "        strat_index = c*IMAGE_PER_CLASS\n",
        "        img_real = images_all[np.random.permutation(class_index[c])[:BATCH_SIZE]]\n",
        "        lab_real = torch.ones((BATCH_SIZE,), device=DEVICE, dtype=torch.long) * c\n",
        "        img_syn = syn_image[strat_index: strat_index + IMAGE_PER_CLASS].reshape((IMAGE_PER_CLASS, channel, im_size[0], im_size[1]))\n",
        "        lab_syn = torch.ones((IMAGE_PER_CLASS,), device=DEVICE, dtype=torch.long) * c\n",
        "        output_real = model(img_real)\n",
        "        loss_real = criterion(output_real, lab_real)\n",
        "        gw_real = torch.autograd.grad(loss_real, net_parameters)\n",
        "        gw_real = list((_.detach().clone() for _ in gw_real))\n",
        "\n",
        "        output_syn = model(img_syn)\n",
        "        loss_syn = criterion(output_syn, lab_syn)\n",
        "        gw_syn = torch.autograd.grad(loss_syn, net_parameters, create_graph=True)\n",
        "        loss += match_mse_loss(gw_syn, gw_real)\n",
        "\n",
        "      total_loss += loss\n",
        "      optimizer_img.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer_img.step()\n",
        "\n",
        "      image_syn_train, label_syn_train = copy.deepcopy(syn_image.detach()), copy.deepcopy(syn_lable.detach())  # avoid any unaware modification\n",
        "      train_model(model, image_syn_train, label_syn_train, NUMBER_STEP_OPT)\n",
        "      \n",
        "  random = \"Gaussian noise\" if randmon_init else \"real training images\"\n",
        "  save_name = os.path.join(SAVEPATH, 'vis_%s_%s_%s_%d.png'%(model_name, dataset_name, random, IMAGE_PER_CLASS))\n",
        "  image_syn_vis = copy.deepcopy(syn_image.detach().cpu())\n",
        "  save_image(image_syn_vis, save_name, nrow=IMAGE_PER_CLASS)\n",
        "  make_grid(image_syn_vis, nrow=IMAGE_PER_CLASS)\n",
        "  net_eval = get_network(eval_mode_name, channel, num_classes, im_size).to(DEVICE)\n",
        "  image_syn_eval = copy.deepcopy(syn_image.detach())\n",
        "\n",
        "  print('%s training begins'%get_time())\n",
        "  acc_test = train_and_eval(net_eval, image_syn_eval, syn_lable, dst_test)\n",
        "  print('%s training end'%get_time())\n",
        "  return acc_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlDU1mMUHB0o"
      },
      "source": [
        "Condense MINIST with ConvNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGdPCX2vG6PJ",
        "outputId": "11abace4-7d35-45a1-a0d4-8cafde57a173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-12-12 08:06:36] training begins\n",
            "[2022-12-12 08:22:42] training begins\n",
            "Epoch: 0 \tTraining Cost: 2.351295\t Valid Acc: 0.18271484971046448.\n",
            "Epoch: 1 \tTraining Cost: 2.254751\t Valid Acc: 0.35009765625.\n",
            "Epoch: 2 \tTraining Cost: 2.188896\t Valid Acc: 0.4886718690395355.\n",
            "Epoch: 3 \tTraining Cost: 2.130546\t Valid Acc: 0.5750976800918579.\n",
            "Epoch: 4 \tTraining Cost: 2.076948\t Valid Acc: 0.6249023675918579.\n",
            "Epoch: 5 \tTraining Cost: 2.026360\t Valid Acc: 0.6583008170127869.\n",
            "Epoch: 6 \tTraining Cost: 1.979302\t Valid Acc: 0.680859386920929.\n",
            "Epoch: 7 \tTraining Cost: 1.935225\t Valid Acc: 0.6963867545127869.\n",
            "Epoch: 8 \tTraining Cost: 1.894252\t Valid Acc: 0.7093750238418579.\n",
            "Epoch: 9 \tTraining Cost: 1.855789\t Valid Acc: 0.719433605670929.\n",
            "Epoch: 10 \tTraining Cost: 1.819694\t Valid Acc: 0.725878894329071.\n",
            "Epoch: 11 \tTraining Cost: 1.785347\t Valid Acc: 0.732421875.\n",
            "Epoch: 12 \tTraining Cost: 1.752800\t Valid Acc: 0.73828125.\n",
            "Epoch: 13 \tTraining Cost: 1.721762\t Valid Acc: 0.7413086295127869.\n",
            "Epoch: 14 \tTraining Cost: 1.692049\t Valid Acc: 0.7469726800918579.\n",
            "Epoch: 15 \tTraining Cost: 1.663683\t Valid Acc: 0.75.\n",
            "Epoch: 16 \tTraining Cost: 1.636448\t Valid Acc: 0.753222644329071.\n",
            "Epoch: 17 \tTraining Cost: 1.610367\t Valid Acc: 0.756640613079071.\n",
            "Epoch: 18 \tTraining Cost: 1.585223\t Valid Acc: 0.7596679925918579.\n",
            "Epoch: 19 \tTraining Cost: 1.561028\t Valid Acc: 0.762011706829071.\n",
            "[2022-12-12 08:23:18] training end\n",
            "[tensor(0.1827, device='cuda:0'), tensor(0.3501, device='cuda:0'), tensor(0.4887, device='cuda:0'), tensor(0.5751, device='cuda:0'), tensor(0.6249, device='cuda:0'), tensor(0.6583, device='cuda:0'), tensor(0.6809, device='cuda:0'), tensor(0.6964, device='cuda:0'), tensor(0.7094, device='cuda:0'), tensor(0.7194, device='cuda:0'), tensor(0.7259, device='cuda:0'), tensor(0.7324, device='cuda:0'), tensor(0.7383, device='cuda:0'), tensor(0.7413, device='cuda:0'), tensor(0.7470, device='cuda:0'), tensor(0.7500, device='cuda:0'), tensor(0.7532, device='cuda:0'), tensor(0.7566, device='cuda:0'), tensor(0.7597, device='cuda:0'), tensor(0.7620, device='cuda:0')]\n"
          ]
        }
      ],
      "source": [
        "acc_list = condensed_images_and_evaluate(\"MNIST\", \"ConvNet\", \"ConvNet\")\n",
        "print(acc_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWrP9ds9HR1q"
      },
      "source": [
        "\n",
        "Condense CIFAR10 with ConvNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDwDiLHG5l8",
        "outputId": "c7b3c188-d22f-48f5-db76-4e90a7c1fc33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[2022-12-12 08:25:02] training begins\n",
            "[2022-12-12 08:41:22] training begins\n",
            "Epoch: 0 \tTraining Cost: 2.320782\t Valid Acc: 0.11757812649011612.\n",
            "Epoch: 1 \tTraining Cost: 2.281565\t Valid Acc: 0.12724609673023224.\n",
            "Epoch: 2 \tTraining Cost: 2.255727\t Valid Acc: 0.12900391221046448.\n",
            "Epoch: 3 \tTraining Cost: 2.234272\t Valid Acc: 0.13125000894069672.\n",
            "Epoch: 4 \tTraining Cost: 2.214694\t Valid Acc: 0.13457031548023224.\n",
            "Epoch: 5 \tTraining Cost: 2.196168\t Valid Acc: 0.13750000298023224.\n",
            "Epoch: 6 \tTraining Cost: 2.178458\t Valid Acc: 0.13828125596046448.\n",
            "Epoch: 7 \tTraining Cost: 2.161406\t Valid Acc: 0.1396484375.\n",
            "Epoch: 8 \tTraining Cost: 2.145015\t Valid Acc: 0.14199219644069672.\n",
            "Epoch: 9 \tTraining Cost: 2.129116\t Valid Acc: 0.14365234971046448.\n",
            "Epoch: 10 \tTraining Cost: 2.113585\t Valid Acc: 0.14541016519069672.\n",
            "Epoch: 11 \tTraining Cost: 2.098385\t Valid Acc: 0.14824219048023224.\n",
            "Epoch: 12 \tTraining Cost: 2.083453\t Valid Acc: 0.14970703423023224.\n",
            "Epoch: 13 \tTraining Cost: 2.068743\t Valid Acc: 0.15000000596046448.\n",
            "Epoch: 14 \tTraining Cost: 2.054286\t Valid Acc: 0.14990234375.\n",
            "Epoch: 15 \tTraining Cost: 2.040027\t Valid Acc: 0.15107421576976776.\n",
            "Epoch: 16 \tTraining Cost: 2.025931\t Valid Acc: 0.15283203125.\n",
            "Epoch: 17 \tTraining Cost: 2.011927\t Valid Acc: 0.15302734076976776.\n",
            "Epoch: 18 \tTraining Cost: 1.998067\t Valid Acc: 0.15439453721046448.\n",
            "Epoch: 19 \tTraining Cost: 1.984380\t Valid Acc: 0.15615235269069672.\n",
            "[2022-12-12 08:42:03] training end\n",
            "[tensor(0.1176, device='cuda:0'), tensor(0.1272, device='cuda:0'), tensor(0.1290, device='cuda:0'), tensor(0.1313, device='cuda:0'), tensor(0.1346, device='cuda:0'), tensor(0.1375, device='cuda:0'), tensor(0.1383, device='cuda:0'), tensor(0.1396, device='cuda:0'), tensor(0.1420, device='cuda:0'), tensor(0.1437, device='cuda:0'), tensor(0.1454, device='cuda:0'), tensor(0.1482, device='cuda:0'), tensor(0.1497, device='cuda:0'), tensor(0.1500, device='cuda:0'), tensor(0.1499, device='cuda:0'), tensor(0.1511, device='cuda:0'), tensor(0.1528, device='cuda:0'), tensor(0.1530, device='cuda:0'), tensor(0.1544, device='cuda:0'), tensor(0.1562, device='cuda:0')]\n"
          ]
        }
      ],
      "source": [
        "acc_list = condensed_images_and_evaluate(\"CIFAR10\", \"ConvNet\", \"ConvNet\")\n",
        "print(acc_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6yvRVUNFasQ"
      },
      "source": [
        "Task2 d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmWAIXxDIFKS"
      },
      "source": [
        "condense MINIST with ConvNet initialized with Gaussian noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "qArmTBvCFaTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb51a75-2677-4cb5-c1ad-337ae4302f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-12-12 08:44:05] training begins\n",
            "[2022-12-12 09:00:09] training begins\n",
            "Epoch: 0 \tTraining Cost: 2.317122\t Valid Acc: 0.27783203125.\n",
            "Epoch: 1 \tTraining Cost: 2.212241\t Valid Acc: 0.41484376788139343.\n",
            "Epoch: 2 \tTraining Cost: 2.147240\t Valid Acc: 0.5035156607627869.\n",
            "Epoch: 3 \tTraining Cost: 2.089736\t Valid Acc: 0.5556640625.\n",
            "Epoch: 4 \tTraining Cost: 2.037832\t Valid Acc: 0.59423828125.\n",
            "Epoch: 5 \tTraining Cost: 1.989833\t Valid Acc: 0.621386706829071.\n",
            "Epoch: 6 \tTraining Cost: 1.945165\t Valid Acc: 0.6470703482627869.\n",
            "Epoch: 7 \tTraining Cost: 1.903703\t Valid Acc: 0.6670898795127869.\n",
            "Epoch: 8 \tTraining Cost: 1.864957\t Valid Acc: 0.6836914420127869.\n",
            "Epoch: 9 \tTraining Cost: 1.828184\t Valid Acc: 0.696484386920929.\n",
            "Epoch: 10 \tTraining Cost: 1.792968\t Valid Acc: 0.705078125.\n",
            "Epoch: 11 \tTraining Cost: 1.759248\t Valid Acc: 0.714648425579071.\n",
            "Epoch: 12 \tTraining Cost: 1.727322\t Valid Acc: 0.72216796875.\n",
            "Epoch: 13 \tTraining Cost: 1.696894\t Valid Acc: 0.7295898795127869.\n",
            "Epoch: 14 \tTraining Cost: 1.667930\t Valid Acc: 0.7353515625.\n",
            "Epoch: 15 \tTraining Cost: 1.640196\t Valid Acc: 0.7416015863418579.\n",
            "Epoch: 16 \tTraining Cost: 1.613778\t Valid Acc: 0.746874988079071.\n",
            "Epoch: 17 \tTraining Cost: 1.588558\t Valid Acc: 0.7508789300918579.\n",
            "Epoch: 18 \tTraining Cost: 1.564517\t Valid Acc: 0.755664050579071.\n",
            "Epoch: 19 \tTraining Cost: 1.541585\t Valid Acc: 0.7598633170127869.\n",
            "[2022-12-12 09:00:44] training end\n",
            "[tensor(0.2778, device='cuda:0'), tensor(0.4148, device='cuda:0'), tensor(0.5035, device='cuda:0'), tensor(0.5557, device='cuda:0'), tensor(0.5942, device='cuda:0'), tensor(0.6214, device='cuda:0'), tensor(0.6471, device='cuda:0'), tensor(0.6671, device='cuda:0'), tensor(0.6837, device='cuda:0'), tensor(0.6965, device='cuda:0'), tensor(0.7051, device='cuda:0'), tensor(0.7146, device='cuda:0'), tensor(0.7222, device='cuda:0'), tensor(0.7296, device='cuda:0'), tensor(0.7354, device='cuda:0'), tensor(0.7416, device='cuda:0'), tensor(0.7469, device='cuda:0'), tensor(0.7509, device='cuda:0'), tensor(0.7557, device='cuda:0'), tensor(0.7599, device='cuda:0')]\n"
          ]
        }
      ],
      "source": [
        "acc_list = condensed_images_and_evaluate(\"MNIST\", \"ConvNet\", \"ConvNet\", True)\n",
        "print(acc_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO4mDkGsITaX"
      },
      "source": [
        "Condense CIFAR10 with ConvNet initialized with Gaussian noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3OPbn-kIUqE",
        "outputId": "55473f2a-ad79-49f4-a53f-5bc3c73662c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[2022-12-12 09:01:36] training begins\n",
            "[2022-12-12 09:17:57] training begins\n",
            "Epoch: 0 \tTraining Cost: 2.340953\t Valid Acc: 0.10654296725988388.\n",
            "Epoch: 1 \tTraining Cost: 2.293925\t Valid Acc: 0.11083984375.\n",
            "Epoch: 2 \tTraining Cost: 2.264696\t Valid Acc: 0.11376953125.\n",
            "Epoch: 3 \tTraining Cost: 2.240717\t Valid Acc: 0.11572265625.\n",
            "Epoch: 4 \tTraining Cost: 2.218818\t Valid Acc: 0.11884766072034836.\n",
            "Epoch: 5 \tTraining Cost: 2.198152\t Valid Acc: 0.12216796725988388.\n",
            "Epoch: 6 \tTraining Cost: 2.178453\t Valid Acc: 0.12373047322034836.\n",
            "Epoch: 7 \tTraining Cost: 2.159516\t Valid Acc: 0.12763671576976776.\n",
            "Epoch: 8 \tTraining Cost: 2.141193\t Valid Acc: 0.13027344644069672.\n",
            "Epoch: 9 \tTraining Cost: 2.123413\t Valid Acc: 0.13134765625.\n",
            "Epoch: 10 \tTraining Cost: 2.106058\t Valid Acc: 0.13320313394069672.\n",
            "Epoch: 11 \tTraining Cost: 2.089163\t Valid Acc: 0.13496093451976776.\n",
            "Epoch: 12 \tTraining Cost: 2.072722\t Valid Acc: 0.13720703125.\n",
            "Epoch: 13 \tTraining Cost: 2.056603\t Valid Acc: 0.13935546576976776.\n",
            "Epoch: 14 \tTraining Cost: 2.040801\t Valid Acc: 0.1416015625.\n",
            "Epoch: 15 \tTraining Cost: 2.025205\t Valid Acc: 0.14130859076976776.\n",
            "Epoch: 16 \tTraining Cost: 2.009761\t Valid Acc: 0.14208984375.\n",
            "Epoch: 17 \tTraining Cost: 1.994539\t Valid Acc: 0.14453125.\n",
            "Epoch: 18 \tTraining Cost: 1.979562\t Valid Acc: 0.14472655951976776.\n",
            "Epoch: 19 \tTraining Cost: 1.964733\t Valid Acc: 0.14462891221046448.\n",
            "[2022-12-12 09:18:39] training end\n",
            "[tensor(0.1065, device='cuda:0'), tensor(0.1108, device='cuda:0'), tensor(0.1138, device='cuda:0'), tensor(0.1157, device='cuda:0'), tensor(0.1188, device='cuda:0'), tensor(0.1222, device='cuda:0'), tensor(0.1237, device='cuda:0'), tensor(0.1276, device='cuda:0'), tensor(0.1303, device='cuda:0'), tensor(0.1313, device='cuda:0'), tensor(0.1332, device='cuda:0'), tensor(0.1350, device='cuda:0'), tensor(0.1372, device='cuda:0'), tensor(0.1394, device='cuda:0'), tensor(0.1416, device='cuda:0'), tensor(0.1413, device='cuda:0'), tensor(0.1421, device='cuda:0'), tensor(0.1445, device='cuda:0'), tensor(0.1447, device='cuda:0'), tensor(0.1446, device='cuda:0')]\n"
          ]
        }
      ],
      "source": [
        "acc_list = condensed_images_and_evaluate(\"CIFAR10\", \"ConvNet\", \"ConvNet\", True)\n",
        "print(acc_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyDb_SjcMB1o"
      },
      "source": [
        "Task1 3. Cross-architecture Generalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIZfHGovMAZx",
        "outputId": "2abc4d53-279e-4cbd-80de-1a6bbbc0b48e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 0 \tTraining Cost: 2.303045\t Valid Acc: 0.103515625.\n",
            "Epoch: 1 \tTraining Cost: 2.302770\t Valid Acc: 0.10498046875.\n",
            "Epoch: 2 \tTraining Cost: 2.302499\t Valid Acc: 0.10771484673023224.\n",
            "Epoch: 3 \tTraining Cost: 2.302229\t Valid Acc: 0.111328125.\n",
            "Epoch: 4 \tTraining Cost: 2.301961\t Valid Acc: 0.11494141072034836.\n",
            "Epoch: 5 \tTraining Cost: 2.301696\t Valid Acc: 0.11572265625.\n",
            "Epoch: 6 \tTraining Cost: 2.301436\t Valid Acc: 0.11884766072034836.\n",
            "Epoch: 7 \tTraining Cost: 2.301175\t Valid Acc: 0.12236328423023224.\n",
            "Epoch: 8 \tTraining Cost: 2.300918\t Valid Acc: 0.12421875447034836.\n",
            "Epoch: 9 \tTraining Cost: 2.300663\t Valid Acc: 0.12451171875.\n",
            "Epoch: 10 \tTraining Cost: 2.300409\t Valid Acc: 0.12636719644069672.\n",
            "Epoch: 11 \tTraining Cost: 2.300155\t Valid Acc: 0.12705078721046448.\n",
            "Epoch: 12 \tTraining Cost: 2.299904\t Valid Acc: 0.12773437798023224.\n",
            "Epoch: 13 \tTraining Cost: 2.299656\t Valid Acc: 0.12666015326976776.\n",
            "Epoch: 14 \tTraining Cost: 2.299410\t Valid Acc: 0.12744140625.\n",
            "Epoch: 15 \tTraining Cost: 2.299168\t Valid Acc: 0.12646484375.\n",
            "Epoch: 16 \tTraining Cost: 2.298925\t Valid Acc: 0.12685547769069672.\n",
            "Epoch: 17 \tTraining Cost: 2.298685\t Valid Acc: 0.12519530951976776.\n",
            "Epoch: 18 \tTraining Cost: 2.298450\t Valid Acc: 0.12333984673023224.\n",
            "Epoch: 19 \tTraining Cost: 2.298213\t Valid Acc: 0.12246093899011612.\n"
          ]
        }
      ],
      "source": [
        "condensed_images_and_evaluate(\"CIFAR10\", \"ConvNet\", \"AlexNet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_5x46QQMxym",
        "outputId": "ed1e2019-6494-49df-cd63-0f1be0c979e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 \tTraining Cost: 2.302315\t Valid Acc: 0.10664062947034836.\n",
            "Epoch: 1 \tTraining Cost: 2.302051\t Valid Acc: 0.10810547322034836.\n",
            "Epoch: 2 \tTraining Cost: 2.301785\t Valid Acc: 0.10986328125.\n",
            "Epoch: 3 \tTraining Cost: 2.301523\t Valid Acc: 0.1123046875.\n",
            "Epoch: 4 \tTraining Cost: 2.301263\t Valid Acc: 0.11416015774011612.\n",
            "Epoch: 5 \tTraining Cost: 2.301003\t Valid Acc: 0.11591797322034836.\n",
            "Epoch: 6 \tTraining Cost: 2.300747\t Valid Acc: 0.11874999850988388.\n",
            "Epoch: 7 \tTraining Cost: 2.300490\t Valid Acc: 0.12050781399011612.\n",
            "Epoch: 8 \tTraining Cost: 2.300235\t Valid Acc: 0.12216796725988388.\n",
            "Epoch: 9 \tTraining Cost: 2.299982\t Valid Acc: 0.1240234375.\n",
            "Epoch: 10 \tTraining Cost: 2.299733\t Valid Acc: 0.12607422471046448.\n",
            "Epoch: 11 \tTraining Cost: 2.299485\t Valid Acc: 0.12802734971046448.\n",
            "Epoch: 12 \tTraining Cost: 2.299238\t Valid Acc: 0.130859375.\n",
            "Epoch: 13 \tTraining Cost: 2.298994\t Valid Acc: 0.1328125.\n",
            "Epoch: 14 \tTraining Cost: 2.298749\t Valid Acc: 0.13544921576976776.\n",
            "Epoch: 15 \tTraining Cost: 2.298506\t Valid Acc: 0.13798828423023224.\n",
            "Epoch: 16 \tTraining Cost: 2.298267\t Valid Acc: 0.14042969048023224.\n",
            "Epoch: 17 \tTraining Cost: 2.298027\t Valid Acc: 0.14179687201976776.\n",
            "Epoch: 18 \tTraining Cost: 2.297789\t Valid Acc: 0.14443360269069672.\n",
            "Epoch: 19 \tTraining Cost: 2.297553\t Valid Acc: 0.14628906548023224.\n"
          ]
        }
      ],
      "source": [
        "condensed_images_and_evaluate(\"MNIST\", \"ConvNet\", \"AlexNet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task1 4 application"
      ],
      "metadata": {
        "id": "4cynkNXoy146"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opacus==0.15.0\n",
        "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
        "from opacus.privacy_engine import PrivacyEngine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIS9fuItyZDf",
        "outputId": "6419d128-25f7-491f-e4fb-a99f68c354f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opacus==0.15.0\n",
            "  Downloading opacus-0.15.0-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from opacus==0.15.0) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.8/dist-packages (from opacus==0.15.0) (1.13.0+cu116)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.8/dist-packages (from opacus==0.15.0) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->opacus==0.15.0) (4.4.0)\n",
            "Installing collected packages: opacus\n",
            "Successfully installed opacus-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def condensed_images_and_evaluate_priv(dataset_name, model_name, eval_mode_name):\n",
        "  channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(dataset_name, DATAPATH)\n",
        "  class_index = {}\n",
        "  data_save = []\n",
        "  for i in range(len(dst_train)):\n",
        "    lable = dst_train[i][1]\n",
        "    if lable in class_index:\n",
        "      class_index[lable].append(i)\n",
        "    else:\n",
        "      class_index[lable] = [i]\n",
        "  images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
        "  images_all = torch.cat(images_all, dim=0).to(DEVICE)\n",
        "  syn_image = []\n",
        "  for c in range(num_classes):\n",
        "    idx_shuffle = np.random.permutation(class_index[c])[:IMAGE_PER_CLASS]\n",
        "    syn_image.append(images_all[idx_shuffle])\n",
        "  syn_image = torch.cat(syn_image, dim=0).to(DEVICE)\n",
        "  syn_image.requires_grad = True\n",
        "  syn_lable = []\n",
        "  for c in range(num_classes):\n",
        "    syn_lable += [c for i in range(IMAGE_PER_CLASS)]\n",
        "  syn_lable = torch.tensor(syn_lable,device=DEVICE)\n",
        "  optimizer_img = torch.optim.SGD([syn_image, ], lr=LR_IMG)\n",
        "  criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "  total_loss = 0 \n",
        "  print('%s training begins'%get_time())\n",
        "  for exp in range(NUMBER_INITIALIZATIONS):\n",
        "    model = get_network(model_name, channel, num_classes, im_size).to(DEVICE)\n",
        "    model.train()\n",
        "    model_shadow = copy.deepcopy(model)\n",
        "    model_shadow.train()\n",
        "    model_parameters = list(model.parameters())\n",
        "    model_shadow_parameters = list(model_shadow.parameters())\n",
        "    optimizer_net_grad = torch.optim.SGD(model_shadow.parameters(), lr=LR_NET)  # optimizer for obtaining DP real gradient\n",
        "    optimizer_net = torch.optim.SGD(model.parameters(), lr=LR_NET)  # optimizer_net for update model\n",
        "    privacy_engine = PrivacyEngine(model_shadow, sample_size=len(dst_train), batch_size=BATCH_SIZE, alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)), noise_multiplier=1, max_grad_norm=0.1 )\n",
        "    privacy_engine.attach(optimizer_net_grad)\n",
        "    for it in range(ITERATIONS):\n",
        "      loss = torch.tensor(0.0).to(DEVICE)\n",
        "      for c in range(num_classes):\n",
        "        strat_index = c*IMAGE_PER_CLASS\n",
        "        img_real = images_all[np.random.permutation(class_index[c])[:BATCH_SIZE]]\n",
        "        lab_real = torch.ones((BATCH_SIZE,), device=DEVICE, dtype=torch.long) * c\n",
        "        img_syn = syn_image[strat_index: strat_index + IMAGE_PER_CLASS].reshape((IMAGE_PER_CLASS, channel, im_size[0], im_size[1]))\n",
        "        lab_syn = torch.ones((IMAGE_PER_CLASS,), device=DEVICE, dtype=torch.long) * c\n",
        "        model.train()\n",
        "        model_shadow.load_state_dict(model.state_dict()) \n",
        "        model_shadow.train()\n",
        "        model_shadow.zero_grad()\n",
        "        output_real = model_shadow(img_real)\n",
        "        loss_real = criterion(output_real, lab_real)\n",
        "        loss_real.backward()\n",
        "        optimizer_net_grad.step()  \n",
        "        gw_real = list((p.grad.detach().clone() for p in model_shadow_parameters))\n",
        "\n",
        "        output_syn = model(img_syn)\n",
        "        loss_syn = criterion(output_syn, lab_syn)\n",
        "        gw_syn = torch.autograd.grad(loss_syn, model_parameters, create_graph=True)\n",
        "        loss += match_mse_loss(gw_syn, gw_real)\n",
        "\n",
        "      total_loss += loss\n",
        "      optimizer_img.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer_img.step()\n",
        "      image_syn_train, label_syn_train = copy.deepcopy(syn_image.detach()), copy.deepcopy(syn_lable.detach())  # avoid any unaware modification\n",
        "      train_model(model, image_syn_train, label_syn_train, NUMBER_STEP_OPT)\n",
        "      \n",
        "  random = \"Gaussian noise\" if randmon_init else \"real training images\"\n",
        "  save_name = os.path.join(SAVEPATH, 'vis_%s_%s_%s_%d.png'%(model_name, dataset_name, random, IMAGE_PER_CLASS))\n",
        "  image_syn_vis = copy.deepcopy(syn_image.detach().cpu())\n",
        "  save_image(image_syn_vis, save_name, nrow=IMAGE_PER_CLASS)\n",
        "  make_grid(image_syn_vis, nrow=IMAGE_PER_CLASS)\n",
        "  net_eval = get_network(eval_mode_name, channel, num_classes, im_size).to(DEVICE)\n",
        "  image_syn_eval = copy.deepcopy(syn_image.detach())\n",
        "\n",
        "  print('%s training begins'%get_time())\n",
        "  acc_test = train_and_eval(net_eval, image_syn_eval, syn_lable, dst_test)\n",
        "  print('%s training end'%get_time())\n",
        "  return acc_test"
      ],
      "metadata": {
        "id": "uIv4OEdhy94t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "condensed_images_and_evaluate_priv(\"MNIST\", \"ConvNet\", \"ConvNet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdjXOD87zVhL",
        "outputId": "a6d09136-e527-4a18-bed4-2261fc58d5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:752: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:752: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:752: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:752: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:752: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:752: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:752: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:752: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:752: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:752: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \tTraining Cost: 11.076957\t Valid Acc: 0.45909997820854187.\n",
            "Epoch: 1 \tTraining Cost: 8.729900\t Valid Acc: 0.524899959564209.\n",
            "Epoch: 2 \tTraining Cost: 6.572433\t Valid Acc: 0.5862999558448792.\n",
            "Epoch: 3 \tTraining Cost: 4.807154\t Valid Acc: 0.7067999839782715.\n",
            "Epoch: 4 \tTraining Cost: 3.537501\t Valid Acc: 0.6723999977111816.\n",
            "Epoch: 5 \tTraining Cost: 2.377632\t Valid Acc: 0.8036999702453613.\n",
            "Epoch: 6 \tTraining Cost: 1.694393\t Valid Acc: 0.7843999862670898.\n",
            "Epoch: 7 \tTraining Cost: 1.258266\t Valid Acc: 0.7897999882698059.\n",
            "Epoch: 8 \tTraining Cost: 1.003713\t Valid Acc: 0.8093000054359436.\n",
            "Epoch: 9 \tTraining Cost: 0.825141\t Valid Acc: 0.8032000064849854.\n",
            "Epoch: 10 \tTraining Cost: 0.677408\t Valid Acc: 0.7958999872207642.\n",
            "Epoch: 11 \tTraining Cost: 0.586079\t Valid Acc: 0.8047999739646912.\n",
            "Epoch: 12 \tTraining Cost: 0.508415\t Valid Acc: 0.8024999499320984.\n",
            "Epoch: 13 \tTraining Cost: 0.442412\t Valid Acc: 0.8050999641418457.\n",
            "Epoch: 14 \tTraining Cost: 0.398143\t Valid Acc: 0.8025999665260315.\n",
            "Epoch: 15 \tTraining Cost: 0.358092\t Valid Acc: 0.8023999929428101.\n",
            "Epoch: 16 \tTraining Cost: 0.325888\t Valid Acc: 0.7996999621391296.\n",
            "Epoch: 17 \tTraining Cost: 0.296969\t Valid Acc: 0.7985000014305115.\n",
            "Epoch: 18 \tTraining Cost: 0.274039\t Valid Acc: 0.7981999516487122.\n",
            "Epoch: 19 \tTraining Cost: 0.255597\t Valid Acc: 0.7979999780654907.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "task1 5 MHIST"
      ],
      "metadata": {
        "id": "Dw2LFrEk7gpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q8h8qIl7edm",
        "outputId": "edae39a7-5bf9-4476-9554-e2daa4f4af44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "def data_loader(image_dir):\n",
        "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        image_dir+\"train\",\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        color_mode='rgb',\n",
        "        seed=123,\n",
        "        image_size=(224, 224),\n",
        "        batch_size=1)\n",
        "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        image_dir+\"test\",\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        color_mode='rgb',\n",
        "        seed=123,\n",
        "        image_size=(224, 224),\n",
        "        batch_size=1)\n",
        "    return train_ds, test_ds"
      ],
      "metadata": {
        "id": "zmvn8DZdADrj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = data_loader(\"/content/drive/MyDrive/ece1512/images_with_label_majority/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti4Q8iqcAQvr",
        "outputId": "913ef483-b022-4bb8-818d-901a1b805bf9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2175 files belonging to 2 classes.\n",
            "Found 977 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_ft_to_torch(train_ds):\n",
        "  images = []\n",
        "  lables = []\n",
        "  count = 0\n",
        "  for i in train_ds:\n",
        "\n",
        "    count +=1\n",
        "    img = i[0].numpy()\n",
        "    c1 = img[0,:,:,0]\n",
        "    c2 = img[0,:,:,1]\n",
        "    c3 = img[0,:,:,2]\n",
        "    images.append(np.array([c1, c2, c3]))\n",
        "    lables.append(i[1].numpy().argmax())\n",
        "  images = torch.tensor(images,device=DEVICE)\n",
        "  lables = torch.tensor(lables,device=DEVICE)\n",
        "  dst_train = TensorDataset(images, lables)\n",
        "  return dst_train"
      ],
      "metadata": {
        "id": "n5AoyxJ7BVO-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = convert_ft_to_torch(train_ds)\n",
        "test_ds = convert_ft_to_torch(test_ds)"
      ],
      "metadata": {
        "id": "cCJRrOSVC84S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be52bb1-b82a-4840-d851-9a9fbb0ce3d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-fabcd2647853>:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  images = torch.tensor(images,device=DEVICE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model,valid_data):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for data in valid_data:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        output = model(inputs)\n",
        "        output = torch.argmax(output, dim=1)\n",
        "        correct += torch.sum(output == labels)\n",
        "        total += BATCH_SIZE\n",
        "    return correct / float(total)\n",
        "\n",
        "def train_and_eval(model, image_syn_eval, syn_lable, dst_test):\n",
        "  dst_train = TensorDataset(image_syn_eval, syn_lable)\n",
        "  trainloader = torch.utils.data.DataLoader(dst_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  validLoader = torch.utils.data.DataLoader(dst_test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=LR_NET)\n",
        "  cee = nn.CrossEntropyLoss()\n",
        "  valid_acc = 0\n",
        "  for epoch in range(TRAIN_NET_EPOCH):\n",
        "    model.train()\n",
        "    train_loss = 0.\n",
        "\n",
        "    for data in trainloader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = cee(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    valid_acc = evaluate(model, validLoader)\n",
        "    print(\"Epoch: {} \\tTraining Cost: {:.6f}\\t \"\n",
        "        \"Valid Acc: {}.\".format(epoch, train_loss, valid_acc))\n",
        "  return valid_acc\n",
        "\n",
        "def train_model(model, image_syn_eval, syn_lable, epochs):\n",
        "  dst_train = TensorDataset(image_syn_eval, syn_lable)\n",
        "  trainloader = torch.utils.data.DataLoader(dst_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=LR_NET)\n",
        "  cee = nn.CrossEntropyLoss().to(DEVICE)\n",
        "  for e in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.\n",
        "\n",
        "    for data in trainloader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = cee(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def match_mse_loss(gw_syn, gw_real):\n",
        "  gw_real_vec = []\n",
        "  gw_syn_vec = []\n",
        "  for ig in range(len(gw_real)):\n",
        "      gw_real_vec.append(gw_real[ig].reshape((-1)))\n",
        "      gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n",
        "  gw_real_vec = torch.cat(gw_real_vec, dim=0)\n",
        "  gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n",
        "  dis = torch.sum((gw_syn_vec - gw_real_vec) ** 2)\n",
        "  return dis"
      ],
      "metadata": {
        "id": "KP7tLnYyIGKR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_size= (224, 224)\n",
        "channel = 3\n",
        "num_classes = 2\n",
        "dst_train = train_ds\n",
        "dst_test = test_ds\n",
        "randmon_init = False\n",
        "class_index = {}\n",
        "data_save = []\n",
        "BATCH_SIZE = 4\n",
        "for i in range(len(dst_train)):\n",
        "  lable = int(dst_train[i][1])\n",
        "  if lable in class_index:\n",
        "    class_index[lable].append(i)\n",
        "  else:\n",
        "    class_index[lable] = [i]\n",
        "images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
        "images_all = torch.cat(images_all, dim=0).to(DEVICE)\n",
        "\n",
        "syn_image = []\n",
        "for c in range(num_classes):\n",
        "  idx_shuffle = np.random.permutation(class_index[c])[:IMAGE_PER_CLASS]\n",
        "  syn_image.append(images_all[idx_shuffle])\n",
        "syn_image = torch.cat(syn_image, dim=0).to(DEVICE)\n",
        "syn_image.requires_grad = True\n",
        "\n",
        "syn_image = torch.randn(size=(num_classes*IMAGE_PER_CLASS, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=DEVICE)\n",
        "syn_lable = []\n",
        "for c in range(num_classes):\n",
        "  syn_lable += [c for i in range(IMAGE_PER_CLASS)]\n",
        "syn_lable = torch.tensor(syn_lable,device=DEVICE)\n",
        "\n",
        "optimizer_img = torch.optim.SGD([syn_image, ], lr=LR_IMG)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "total_loss = 0 \n",
        "print('%s training begins'%get_time())\n",
        "for exp in range(NUMBER_INITIALIZATIONS):\n",
        "  model = get_network(\"ConvNet\", channel, num_classes, im_size).to(DEVICE)\n",
        "  model.train()\n",
        "  net_parameters = list(model.parameters())\n",
        "\n",
        "  for it in range(ITERATIONS):\n",
        "    loss = torch.tensor(0.0).to(DEVICE)\n",
        "    for c in range(num_classes):\n",
        "      strat_index = c*IMAGE_PER_CLASS\n",
        "      img_real = images_all[np.random.permutation(class_index[c])[:BATCH_SIZE]]\n",
        "      lab_real = torch.ones((BATCH_SIZE,), device=DEVICE, dtype=torch.long) * c\n",
        "      img_syn = syn_image[strat_index: strat_index + IMAGE_PER_CLASS].reshape((IMAGE_PER_CLASS, channel, im_size[0], im_size[1]))\n",
        "      lab_syn = torch.ones((IMAGE_PER_CLASS,), device=DEVICE, dtype=torch.long) * c\n",
        "      output_real = model(img_real)\n",
        "      loss_real = criterion(output_real, lab_real)\n",
        "      gw_real = torch.autograd.grad(loss_real, net_parameters)\n",
        "      gw_real = list((_.detach().clone() for _ in gw_real))\n",
        "\n",
        "      output_syn = model(img_syn)\n",
        "      loss_syn = criterion(output_syn, lab_syn)\n",
        "      gw_syn = torch.autograd.grad(loss_syn, net_parameters, create_graph=True)\n",
        "      loss += match_mse_loss(gw_syn, gw_real)\n",
        "\n",
        "    total_loss += loss\n",
        "    optimizer_img.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_img.step()\n",
        "\n",
        "    image_syn_train, label_syn_train = copy.deepcopy(syn_image.detach()), copy.deepcopy(syn_lable.detach())  # avoid any unaware modification\n",
        "    dst_syn_train = TensorDataset(image_syn_train, label_syn_train)\n",
        "    train_model(model, image_syn_train, label_syn_train, NUMBER_STEP_OPT)\n",
        "    \n",
        "random = \"Gaussian noise\" if randmon_init else \"real training images\"\n",
        "save_name = os.path.join(SAVEPATH, 'vis_%s_%s_%s_%d.png'%(\"ConvNet\", \"MHIST\", random, IMAGE_PER_CLASS))\n",
        "image_syn_vis = copy.deepcopy(syn_image.detach().cpu())\n",
        "save_image(image_syn_vis, save_name, nrow=IMAGE_PER_CLASS)\n",
        "make_grid(image_syn_vis, nrow=IMAGE_PER_CLASS)\n",
        "net_eval = get_network(\"ConvNet\", channel, num_classes, im_size).to(DEVICE)\n",
        "image_syn_eval = copy.deepcopy(syn_image.detach())\n",
        "\n",
        "print('%s training begins'%get_time())\n",
        "acc_test = train_and_eval(net_eval, image_syn_eval, syn_lable, dst_test)\n",
        "print('%s training end'%get_time())"
      ],
      "metadata": {
        "id": "-VNHI0V8-C6C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1675efe4e6db4f3a8fa0ae25aead0558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10aace68830445a48f6bba04eb45e774",
              "IPY_MODEL_ec75cd3d930e4c9080dee52cd8087e8b",
              "IPY_MODEL_1fcccd7fdf4f4363b5b74d871cfd5b34"
            ],
            "layout": "IPY_MODEL_f1fb9e67685a4950b6653ac4da288774"
          }
        },
        "10aace68830445a48f6bba04eb45e774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f3b5d44fe9a40a5b683c35439e630ee",
            "placeholder": "​",
            "style": "IPY_MODEL_770b450752bb43c4a5c4f92240dea42a",
            "value": "100%"
          }
        },
        "ec75cd3d930e4c9080dee52cd8087e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea519938a6784e4ab66615f2ba346780",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d66d787fbf7146f6ad8efa9f125dce63",
            "value": 170498071
          }
        },
        "1fcccd7fdf4f4363b5b74d871cfd5b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_048da1980d71475d98fed0fb3cf74689",
            "placeholder": "​",
            "style": "IPY_MODEL_cb9ce157239b4b25aee114f70843c723",
            "value": " 170498071/170498071 [00:14&lt;00:00, 13947230.51it/s]"
          }
        },
        "f1fb9e67685a4950b6653ac4da288774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f3b5d44fe9a40a5b683c35439e630ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "770b450752bb43c4a5c4f92240dea42a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea519938a6784e4ab66615f2ba346780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d66d787fbf7146f6ad8efa9f125dce63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "048da1980d71475d98fed0fb3cf74689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb9ce157239b4b25aee114f70843c723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}